# env variables needed
# NGC_API_KEY

services:
  # nv-embedqa-e5-v5:
  #   image: "nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1"
  #   profiles: ["Local LLM + Embedding", "Local LLM + Embedding + Reranking"]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: ["gpu"]
  #             count: 1
  #   ipc: host
  #   environment:
  #     - NGC_API_KEY=${NGC_API_KEY}
  #   volumes:
  #     - nim-cache:/opt/nim/.cache
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import requests; resp = requests.get('http://localhost:8000/v1/health/ready'); resp.raise_for_status()"]
  #     interval: 30s
  #     start_period: 600s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - default

  # nv-rerankqa-mistral-4b-v3:
  #   image: "nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2"
  #   profiles: ["Local LLM + Embedding + Reranking"]
  #   runtime: "nvidia"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: ["gpu"]
  #             count: 1
  #   ipc: host
  #   environment:
  #     - NGC_API_KEY=${NGC_API_KEY}
  #   volumes:
  #     - nim-cache:/opt/nim/.cache
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import requests; resp = requests.get('http://localhost:8000/v1/health/ready'); resp.raise_for_status()"]
  #     interval: 30s
  #     start_period: 600s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - default

  # llm-nim:
  #   image: "nvcr.io/nim/meta/llama-3.2-1b-instruct:latest" 
  #   profiles: ["Local LLM", "Local LLM + Embedding", "Local LLM + Embedding + Reranking"]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: ["gpu"]
  #             count: 1
  #   ipc: host
  #   environment:
  #     - NGC_API_KEY=${NGC_API_KEY}
  #   volumes:
  #     - nim-cache:/opt/nim/.cache
  #   healthcheck:
  #     test: ["CMD", "python", "-c", "import requests; resp = requests.get('http://localhost:8000/v1/health/ready'); resp.raise_for_status()"]
  #     interval: 30s
  #     start_period: 600s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - default

  milvus:
    image: "milvusdb/milvus:v2.4.6"
    security_opt:
      - seccomp:unconfined
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - COMMON_STORAGETYPE=local
    volumes:
      - milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    command: "milvus run standalone"
    networks:
      - default

  redis:
    image: "redis:7"
    volumes:
      - redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      start_period: 30s
      timeout: 5s
      retries: 5
    command: "redis-server --save 20 1 --loglevel warning"
    networks:
      - default

  # Add a chain-server service with proper environment variables
  chain-server:
    image: "python:3.10-slim"
    volumes:
      - ./code:/app
    working_dir: /app
    command: ["python", "-m", "chain_server"]
    environment:
      # Use the Docker host's network IP instead of hostname
      - APP_LLM_MODEL__URL=http://172.17.0.1:8001/v1
      - APP_EMBEDDING_MODEL__URL=http://172.17.0.1:8000/v1
      - APP_RERANKING_MODEL__URL=http://172.17.0.1:8002/v1
      - APP_LLM_MODEL__NAME=meta/llama-3.2-1b-instruct
      - APP_EMBEDDING_MODEL__NAME=nvidia/llama-3.2-nv-embedqa-1b-v2
      - APP_RERANKING_MODEL__NAME=nvidia/llama-3.2-nv-rerankqa-1b-v2
      - APP_REDIS_DSN=redis://redis:6379/0
      - APP_MILVUS__URL=http://milvus:19530
      - APP_LOG_LEVEL=DEBUG
    depends_on:
      - milvus
      - redis
    ports:
      - "8080:8080"

networks:
  default:

volumes:
  milvus:
  redis:
  nim-cache:

